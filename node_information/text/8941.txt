   #publisher Medium alternate

   The Guardian
   Sign in
   (BUTTON) Get started

   The Guardian

Facial Recognition Will Soon Be Everywhere. Are We Prepared?

Some companies are already testing this new technology, but it raises
questions about how surveillance can be abused

   The Guardian
   The Guardian
   (BUTTON) Follow
   May 21 · 4 min read
   A demonstration uses artificial intelligence and facial recognition in
   dense crowd at the Horizon Robotics exhibit at the Las Vegas Convention
   Center during CES 2019. Photo: David McNew/AFP/Getty Images

   By Dylan Curran

   Imagine this: you walk into work and the camera above the doors scans
   your face, opening them seamlessly without you lifting a…

This story is just a click away.

   Sign up for a free Medium account to read this story from The Guardian.
   Plus, get one more story in your member preview this month.
   Sign up with Google
   Sign up with Facebook
   Already have an account? Sign in
     * Privacy
     * Technology
     * Facial Recognition
     * Face Recognition
     * Artificial Intelligence

(BUTTON) 137 claps

   (BUTTON)
   (BUTTON)
   The Guardian

   Written by

The Guardian

   (BUTTON) Follow

Top stories, special features, live blogs and more

   (BUTTON) Follow
   The Guardian

The Guardian

   (BUTTON) Follow

The world’s leading liberal voice, since 1821

   (BUTTON) Follow
   See responses (2)

Discover Medium

   Welcome to a place where words matter. On Medium, smart voices and
   original ideas take center stage - with no ads in sight. Watch

Make Medium yours

   Follow all the topics you care about, and we’ll deliver the best
   stories for you to your homepage and inbox. Explore

Become a member

   Get unlimited access to the best stories on Medium — and support
   writers while you’re at it. Just $5/month. Upgrade
   AboutHelpLegal
   #publisher Medium alternate

   Become a member
   Sign in
   (BUTTON) Get started

Month 2 Master: Mastering 12 skills in 12 months

   Jeffrey Li
   Jeffrey Li
   (BUTTON) Follow
   Dec 31, 2017 · 7 min read
   Source: xyleme.com

   After reading the Happiness Pursuit & seeing Max Deutsch’s own
   ambitious M2M challenge, I’ve decided to take on my most ambitious
   “quest” yet. I will replicate Max’s M2M project and spend the next 12
   months attempting to master 12 different skills. I had already been
   doing this with accelerated learning projects such as freestyle
   rapping, copywriting, poker, programming. However, I never formalized
   it into a project nor did I document the process. Seeing Max’s project,
   brought to mind Ralph Waldo Emerson’s quote: “in every work of genius
   we recognize our own rejected thoughts; they come back to us with a
   certain alienated majesty.”

   Whether it’s hiking 55 miles in 2.5 days, going on 100 mile+ weekend
   bike rides, swimming from SF Alcatraz island, silent meditation
   retreats, going “urban camping”, freestyle rapping, I feel alive when I
   push the limits of what I can do.

   The goal here, is to see how far I can take my skillset with only a
   month of rapid skill development. I’ll also be testing & iterating upon
   my own framework for learning anything. You can learn anything you
   want, while having a full-time job, social life + other life
   obligations.

   The ability to learn is a superpower and once we cultivate this
   superpower, we can get whatever we want in our lives. If we can
   drastically improve learning, equip children & adults, with the
   meta-skill of being able to acquire any skill, we’ll make exponential
   progress on all world problems.

   Throughout this process, I’ll write about my progress daily on this
   medium account to force public accountability. The purpose of this
   post, will be to outline the skills I plan to master + rules/guidelines
   for this challenge. Not only will this be a challenge in learning, this
   will be a challenge in self-discipline.

The Accelerated Learning Challenges

January — Social Skills

   Goal: Overcome the fear of difficult social situations, such as making
   friends with groups of strangers, meeting attractive women and being
   social in general.

   Daily strategy plan is unavailable.

   Click here for daily log.

   Day 30 Result — 100 Stranger Interactions:

   IFRAME:
   https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtub
   e.com%2Fembed%2Fshma1f6QOTU%3Ffeature%3Doembed&url=http%3A%2F%2Fwww.you
   tube.com%2Fwatch%3Fv%3Dshma1f6QOTU&image=https%3A%2F%2Fi.ytimg.com%2Fvi
   %2Fshma1f6QOTU%2Fhqdefault.jpg&key=a19fcc184b9711e1b4764040d3dc5c07&typ
   e=text%2Fhtml&schema=youtube

February — Pull-Ups

   Goal: Complete a set of 40 consecutive, full-range motion pull-ups.

   Click here for the learning strategy plan.

   Click here for daily log.

   Day 30 Result: 27/40 completed. 500 Pull-Ups in a Day

   IFRAME:
   https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtub
   e.com%2Fembed%2FzGhoXqHo1K0%3Ffeature%3Doembed&url=http%3A%2F%2Fwww.you
   tube.com%2Fwatch%3Fv%3DzGhoXqHo1K0&image=https%3A%2F%2Fi.ytimg.com%2Fvi
   %2FzGhoXqHo1K0%2Fhqdefault.jpg&key=a19fcc184b9711e1b4764040d3dc5c07&typ
   e=text%2Fhtml&schema=youtube

March — Deep Learning/Artificial Intelligence

   Goal: Build and train an artificial intelligence to automate online
   dating.

   Click here for the learning strategy plan.

   Click here for daily log.

   Day 30 Result: Tinder successfully automated.

April — Card Memorization

   Goal: Memorize the order of randomly shuffled deck of cards in under 2
   minutes.

   Click here for the learning strategy plan.

   Click here for daily log.

   Day 27 Result: 3 minutes 14 seconds, 49/52 correct

   IFRAME:
   https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtub
   e.com%2Fembed%2F6MbB_zH5Yek%3Ffeature%3Doembed&url=http%3A%2F%2Fwww.you
   tube.com%2Fwatch%3Fv%3D6MbB_zH5Yek&image=https%3A%2F%2Fi.ytimg.com%2Fvi
   %2F6MbB_zH5Yek%2Fhqdefault.jpg&key=a19fcc184b9711e1b4764040d3dc5c07&typ
   e=text%2Fhtml&schema=youtube

May — Language Learning (Spanish)

   Goal: Go on a “Spanish-only” date with a non-english speaking women in
   Medellin, Colombia

   Click here for the learning strategy plan.

   Click here for daily log.

   Day 30 Result: Success!

   IFRAME:
   https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtub
   e.com%2Fembed%2FFATwg13sEJM%3Ffeature%3Doembed&url=http%3A%2F%2Fwww.you
   tube.com%2Fwatch%3Fv%3DFATwg13sEJM&image=https%3A%2F%2Fi.ytimg.com%2Fvi
   %2FFATwg13sEJM%2Fhqdefault.jpg&key=a19fcc184b9711e1b4764040d3dc5c07&typ
   e=text%2Fhtml&schema=youtube

June — Guitar (Finger-Style)

   Goal: Become a Guitar Playing Prodigy by re-creating Sunga Jung’s One
   Summer’s Day

   Click here for the learning strategy plan.

   Click here for daily log.

   Day 30 Result: Success! Strong bar chords in a month is tough.

   IFRAME:
   https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtub
   e.com%2Fembed%2FRtO6ItTLqoQ%3Ffeature%3Doembed&url=http%3A%2F%2Fwww.you
   tube.com%2Fwatch%3Fv%3DRtO6ItTLqoQ&image=https%3A%2F%2Fi.ytimg.com%2Fvi
   %2FRtO6ItTLqoQ%2Fhqdefault.jpg&key=a19fcc184b9711e1b4764040d3dc5c07&typ
   e=text%2Fhtml&schema=youtube

July — Cooking

   Goal: Complete the “World Cup of Cooking” challenge.

   Click here for the learning strategy plan.

   Click here for daily log.

   Day 30 Result: Success!

August — Surfing in Costa Rica

   Goal: Complete the BTCC Surfing Challenge.

   Click here for the learning strategy plan.

   Click here for the daily log.

   Day 30 Result: Kinda Success!

   IFRAME:
   https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtub
   e.com%2Fembed%2FHd7gSIbsRIA%3Ffeature%3Doembed&url=http%3A%2F%2Fwww.you
   tube.com%2Fwatch%3Fv%3DHd7gSIbsRIA&image=https%3A%2F%2Fi.ytimg.com%2Fvi
   %2FHd7gSIbsRIA%2Fhqdefault.jpg&key=a19fcc184b9711e1b4764040d3dc5c07&typ
   e=text%2Fhtml&schema=youtube

September—Popping

   Goal: Complete a 3 minute, non-stop popping choreography & 3 minute
   battle freestyle.

   Click here for the learning strategy plan.

   Click here for the daily log.

   Day 30 Result: Success!

   IFRAME:
   https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtub
   e.com%2Fembed%2F9UNFAhe4Siw%3Ffeature%3Doembed&url=http%3A%2F%2Fwww.you
   tube.com%2Fwatch%3Fv%3D9UNFAhe4Siw&image=https%3A%2F%2Fi.ytimg.com%2Fvi
   %2F9UNFAhe4Siw%2Fhqdefault.jpg&key=a19fcc184b9711e1b4764040d3dc5c07&typ
   e=text%2Fhtml&schema=youtube

October— iOS Development

   Goal: Build and deploy an iOS application that helps me freestyle rap.

   Click here for the learning strategy plan.

   Click here for the daily log.

   Day 30 Result: Built the app, didn’t get accepted into the app store :(

   IFRAME:
   https://cdn.embedly.com/widgets/media.html?url=http%3A%2F%2Fwww.youtube
   .com%2Fwatch%3Fv%3DxrIgYoC2P6c&src=https%3A%2F%2Fwww.youtube.com%2Fembe
   d%2FxrIgYoC2P6c&type=text%2Fhtml&key=a19fcc184b9711e1b4764040d3dc5c07&s
   chema=youtube

November — Drawing (Portrait)

   Goal: Draw one realistic portrait.

   Click here for the learning strategy plan.

   Click here for the daily log.

   Day 30 Result: Success!

December — Entrepreneurship

   Goal: Start a Profitable Business within 30 Days with under $100 of
   capital

   Click here for the learning strategy plan.

   Click here for the daily log.

   Day 30 Result: Data Science Interviews Product launched! Not profitable
   yet, but did make my first dollar online!

The Rules/Principles

   With the challenges in place, it’ll be important to establish a few
   rules before I start. By pre-defining what I can and cannot do
   beforehand, this’ll prevent me from falling into the inevitable “grey”
   area when complications arise:
    1. I’m allowed to change my challenges with new information. However,
       I can only do this before the challenge starts. This is to prevent
       my lower self from giving up when things get tough. Before I start
       is OK, since there are too many unknown variables planning twelve
       months out. I’m also allowed to change the challenge if I find it
       too easy.
    2. I intend to work on the skill everyday for the next year. However,
       there are only two exceptions: 1) In March, I’ll only have 27 days
       to complete my challenge since I’ll be attending Tony Robbins’ UPW
       2) If I feel that I’m burning out & need a rest day, I’m allowed to
       take it since burning out will cause me to be further from
       accomplishing my goal. Burnout will also hurt my ability to
       complete other challenges.
    3. Do not sacrifice quality of work for the completion of the
       challenge. It’s important to keep priorities straight. Doing great
       work is what gives me the lifestyle & money to be able to complete
       these challenges. Do not get it mixed up.
    4. Alcoholic intake will be minimized to 2 drinks maximum per social
       outing. Any binge drinking destroys my ability to practice & the
       good habits I set for myself. The easiest productivity hack I’ve
       found, is to stop drinking. However, alcohol is ingrained in the
       fabric of most social interactions. Two drinks maximum for parties,
       dates, happy hours etc.
    5. Always lean towards honesty/truth over the better story. There’s a
       possibility that I may do a challenge and realize it’s too
       difficult. Never twist the story to make it sound like a success.
       Choose failure & truth over dishonesty.
    6. Schedule my social life in advance and build my practice time
       around it. If I have nothing to do, I have a habit of spending all
       my time on my personal projects. Although this is generally a good
       thing, being alone working on my own stuff is the easiest way to
       get stuck in my head and spiral into unhappiness. Knowing this
       about myself, I’ll need to deliberately schedule social time.
    7. Vacation: There might be times where I’ll need a vacation. I’ll
       take a vacation during the months where I have a skill that
       logistically allows me to practice on vacation. If I really need a
       vacation, then I’ll switch the skill before the challenge starts to
       something more logistically feasible.

   Follow this Medium Account to follow my journey. I’ll be posting
   updates daily. Starting tomorrow, January 1st, I’ll be starting my
   first challenge: Social Skills.

   Inspirations for this project:
     * Max Deutsch’s M2M Project
     * Scott Young’s MIT Challenge & Year Without English
     * Girl Learns to Dance in a Year
     * Connor Grooms learns Spanish in a month
     * Kid learns programming using a Nokia feature phone
     * Sandoche’s learn 12 topics in 12 months project @ Learning Lab
     * Pieter Levels’ 12 Startups in 12 Months
     * Jennifer Dewalt’s 180 Websites in 180 Days

Jeff Li is saving the world by matrix-downloading skills into his brain and
taking NZT (Limitless) pills. He uses these skills for good. He is …….“The
SuperLearner. ”

If you love me and this project, follow this medium account. If you hate me,
you should pretend to love me and still follow this medium account. There’s
only one option here……

(BUTTON) 425

     * Education
     * Learning
     * Accelerated Learning
     * Personal Growth

(BUTTON) 425 claps

   (BUTTON)
   (BUTTON)
   Jeffrey Li

   Written by

Jeffrey Li

   (BUTTON) Follow

Accelerated Learning Fanatic | Data Scientist | Educator

   (BUTTON) Follow
   See responses (5)

Discover Medium

   Welcome to a place where words matter. On Medium, smart voices and
   original ideas take center stage - with no ads in sight. Watch

Make Medium yours

   Follow all the topics you care about, and we’ll deliver the best
   stories for you to your homepage and inbox. Explore

Become a member

   Get unlimited access to the best stories on Medium — and support
   writers while you’re at it. Just $5/month. Upgrade
   AboutHelpLegal
   #publisher Medium alternate

   Become a member
   Sign in
   (BUTTON) Get started

How I Eat For Free in NYC Using Python, Automation, Artificial Intelligence,
and Instagram

   Chris Buetti
   Chris Buetti
   (BUTTON) Follow
   Feb 24 · 22 min read

   Living and working in the big apple comes with big rent.

   I, along with most other city-dwellers who live inside a crammed closet
   we call an apartment, look to cut costs anywhere we can. It’s no secret
   one way to curtail expenses, at least we’re told, is to cook at home
   instead of eating out all of the time. As a Hell’s Kitchen resident
   this is near impossible. Everywhere I look there is a sushi bar,
   Mexican restaurant or some delicious looking pizzeria within arm’s
   length that can break my willpower in the blink of an eye. I fall
   victim to this way more than I’d like to admit. Well, I used to fall
   victim to this — until recently. Not wanting to give up the dining
   experiences I enjoyed so dearly, I decided I’d create my own currency
   to finance these transactions. I’ve been dining at restaurants,
   sandwich shops, and other eateries for free ever since.

   I’m going to explain to you how I’m receiving these free meals from
   some of the best restaurants in New York City. I’ll admit — it’s rather
   technical and not everyone can reproduce my methodology. You’ll either
   need a background in Data Science/Software Development or a lot of free
   time on your hands. Since I have the prior, I sit back and let my code
   do the work for me. Oh, and you guessed it, you’ll need to know how to
   use Instagram as well.

   If you’re part of the technical audience, I will briefly go over some
   of the technologies and programming languages I use but I will not be
   providing code or anything like that. I will explain my use of logistic
   regression, random forests, AWS, and automation — but not in depth.
   This article will be more theory based. If you’re a non-technical
   reader, everything here can still be done, it’s just going to take some
   time and effort. These methods are tedious which is why I decided to
   automate most of them.

   Now to get into it. I’ll start with the answer and then go through how
   I got there.

What I did

   In today’s digital age, a large Instagram audience is considered a
   valuable currency. I had also heard through the grapevine that I could
   monetize a large following — or in my desired case — use it to have my
   meals paid for. So I did just that.

   I created an Instagram page that showcased pictures of New York City’s
   skylines, iconic spots, elegant skyscrapers — you name it. The page has
   amassed a following of over 25,000 users in the NYC area and it’s still
   rapidly growing.

   I reach out restaurants in the area either via Instagram’s direct
   messaging or email and offer to post a review to my followers in return
   for a free entree or at least a discount. Almost every restaurant I’ve
   messaged came back at me with a compensated meal or a gift card. Most
   places have an allocated marketing budget for these types of things so
   they were happy to offer me a free dining experience in exchange for a
   potential promotion. I’ve ended up giving some of these meals away to
   my friends and family because at times I had too many queued up to use
   myself.

   The beauty of this all is that I automated the whole thing. And I mean
   100% of it. I wrote code that finds these pictures or videos, makes a
   caption, adds hashtags, credits where the picture or video comes from,
   weeds out bad or spammy posts, posts them, follows and unfollows users,
   likes pictures, monitors my inbox, and most importantly — both direct
   messages and emails restaurants about a potential promotion. Since its
   inception, I haven’t even really logged into the account. I spend zero
   time on it. It’s essentially a robot that operates like a human, but
   the average viewer can’t tell the difference. And as the programmer, I
   get to sit back and admire its (and my) work.

How I Did It

   I’ll walk you through how I did what I did, from A all the way to Z.
   Some of this may seem like common sense, but when you’re automating a
   system to act like a human, details are important. The process can be
   broken down into three phases: content sharing, growth hacking, and
   sales & promotion.

The Content

   Now, none of the content my account posts is owned by me. I re-share
   other peoples content on my page, with credit to them. If someone asks
   me to take down their photo, I do immediately. But since I am sourcing
   their page, I’ve only been thanked — never the opposite.

   Posting every day — multiple times a day — is indispensable. This is
   one of the main factors the Instagram algorithm uses to determine how
   much they are going to expose you to the public (via the “explore
   page”). Posting every day, especially at “rush hour” times, is much
   harder and more monotonous than you might think. Most people give up on
   this task after a few weeks, and even missing a day or two can be
   detrimental. So, I automated the content collecting and sharing
   process.
     * Getting pictures and videos in inventory

   I first thought about setting up a picture scraper from Google Images
   or from Reddit to get my content. One of the biggest struggles I came
   across was how particular Instagram is with the sizing of the picture
   being posted. Ideally, it’s a “square” picture, meaning its width
   equals its height, so it will reject an out-of-proportion post attempt.
   This made retrieving content very challenging.

   I ultimately decided to scrape directly from other Instagram feeds
   because the picture will come in precisely the right size as it is. It
   also allows me to know exactly where the picture came from, which will
   come in handy during the auto-crediting process.

   I collected a list of fifty other Instagram accounts that posted
   quality pictures of NYC. Using some opensource software, I set up a
   scraper to go through and download media from these other accounts. In
   addition to the actual content, I scraped a bunch of metadata along
   with the picture such as the caption, the number of likes, and the
   location. I set the scraper to run every morning at 3:00 AM or when my
   inventory was empty.

   From this, I now have a central location with related content in the
   right format.
     * Automatically deciding what’s “good” or “bad” content

   Not everything someone posts on Instagram is re-sharable. A lot of the
   time people are trying to sell something, shouting another page, or it
   could flat out just be bad or unrelated content. Take these two posts
   as an example:

   The above two posts are from the same NYC-based Instagram account. The
   one on the left is a normal natural post in their niche — one I would
   be happy re-sharing on my page. The one on the right, however, is an
   advertisement. Without any context, if I put this on my page it would
   be rather confusing and out of place. The caption got cut off, but it’s
   actually promoting a NYC-based app. You can see the difference in the
   number of likes — 8200 vs. 1000. I need to be able to automatically
   weed out posts like those on the right, and re-share posts like that of
   the left.

   Therefore, I can’t just blindly re-share all the content that I scrape.
   And since this will be an automated process, I needed to create an
   algorithm that can weed out the bad from the good. The first part of my
   “cleaner” has some hard-coded rules and the second is a machine
   learning model that refines the content even further.

   Cleaner Part 1 — Hard Coded Rules:

   The first thing I did was refine my inventory on some specific
   guidelines from the metadata. I was rather strict because there is no
   shortage of content for me to share. If there was even a slight
   red-flag, I trashed the picture. I can always scrape more content, but
   if my algorithm posts something spammy or inappropriate to my page
   there may be thousands of people that see it before I recognize and
   remove it.

   The preliminary step was to have my algorithm look at the caption. If
   the text includes any text related to “link in bio”, “buy now”,
   “limited time”, or the related, I immediately have it fail the test.
   These are typical of posts looking to sell something rather than
   quality content for entertainment purposes.

   The next thing I looked at is if the comments were disabled. If they
   were, I failed the picture. Disabled comments from my experience were
   linked to controversial posts and not worth the risk.

   The final thing I looked at was if there was more than one person
   tagged in the picture. A lot of the times, one tag in a picture is a
   credit to where it came from, so I actually found that to be
   beneficial. But if the picture had multiple tags, It would lead to
   confusion when it came to crediting or what the purpose of the post
   even was.

   From these rules, I was able to get most of the spammy and undesirable
   posts into the trash and out of my folder. However, just because a post
   isn’t trying to sell something doesn’t mean it’s a good, quality post.
   Also, my hard-coded rules may still miss some sales-y content, so I
   wanted to run them through a secondary model once I was done with part
   one.

   Cleaner Part 2— Machine Learning Model:

   As I was going through my now-cleaner repository of pictures, I noticed
   there were still some lingering items that weren’t particularly
   desirable to post. I wasn’t going to be able to sit there and manually
   move out the bad ones as I planned for this to be completely automated.
   I wanted to run each through another test.

   I had a ton of metadata on each of the posts, including the number of
   likes, captions, time of post, and much more. My original goal was to
   try to predict which pictures would garner the most likes. However, the
   issue was that bigger accounts naturally had more likes so it wasn’t a
   fair barometer. My follow-up thought would be to make the response
   variable equal to the like ratio (number of likes/number of followers)
   and try to predict that. After looking at each picture and its
   respective ratio, I still didn’t trust the correlation. I didn’t feel
   those with higher ratios were necessarily the best photos. Just because
   an account was “popular” didn’t mean it had better content than a
   relatively unknown photographer with fewer likes. I decided to change
   my outlook from a regression model to a classification model and just
   decide if the picture is good enough to post or not — a simple yes or
   no.

   Before even looking at any of the other metadata, I scraped a large
   number of photos and manually went through them, labeling them as 0
   (bad) or 1 (good). This is extremely subjective, so I’m theoretically
   making a model to my own conscious. However, it seems to be pretty
   universally agreed upon as to which content is unappealing and which is
   favorable.

   I generated my own dataset. The response variable was 0/1 (bad/good)
   with a large number of features. The metadata of each post gave me the
   following information:
Caption
Number of likes
Number of comments
Picture or video
Video views (if applicable)
When the picture was posted
Number of followers of the account that posted it

   From these seven explanatory variables, I engineered a few more
   features that I thought would be useful. For example, I changed the
   number of comments and likes to ratios against followers. I extracted
   the number of hashtags from the caption and made that its own column,
   and did the same with the number of accounts mentioned in the caption.
   I cleaned up and vectorized the rest of the caption to be used in
   Natural Language Processing. Vectorizing is the process of removing
   peripheral words (“the”, “and”, etc.) and converting the remaining into
   a numeric field that can be analyzed mathematically. After all was said
   and done, I had the resulting data:
Response Variable:
Post Rating (0/1)Explanatory Variables:
Vectorized caption
Number of mentions
Number of hashtags
Caption length
If the caption was edited from its origin
Media type
Video view ratio/number of days since it was posted
Like ratio/number of days since it was posted
Comment ratio/number of days since it was posted

   I played around with a number of classification algorithms such as
   Support Vector Machines and Random Forests but landed on a basic
   Logistic Regression. I did this for a few reasons, first being Occam’s
   Razor — sometimes the simplest answer is the right one. No matter which
   way I spun or re-engineered the data, logistic regression performed the
   best on my test set. The second and more important reason was that,
   unlike some other classification algorithms, I can set a threshold
   score while making predictions. It’s common for classification
   algorithms to output a binary class (in my case 0 or 1) but logistic
   regression actually yields a decimal between 0 and 1. For example, it
   may rate a post as 0.83 or 0.12. It’s common to set the threshold at
   0.5 and rank everything greater than that to 1 and everything else to
   0, but it would be case dependent. Since this task is critical, and
   there is an abundance of media available, I was extremely strict on my
   threshold and set it to 0.9 and rejected anything that fell below that
   benchmark.

   After I implemented my model, the inventory of pictures and videos was
   A) cleaned by a hard set of rules and then B) only the cream of the
   crop was chosen by my logistic regression algorithm. I am now able to
   move on to captioning and crediting each post.

Auto-Captioning and Auto-Crediting

   I now had a system of automatically gathering relevant content and
   removing the tangential or spammy images— but I’m not done yet.

   If you’ve used Instagram before, you know that each post has a caption
   that exists under the picture or video. Being that I can’t actually see
   these pictures, nor do I have the time to sit there and caption them
   all, I needed to make a generic caption that can be used for any of
   them.

   The first thing I did was make a final template. It looked something
   like this:
{CAPTION}
.
.
.
Credit: {CREDIT}
.
.
.
{HASHTAG1 HASHTAG2 ... HASHTAG30}

   Where the three sets of {}’s needed to be filled in by my script. Let’s
   go through each three one-by-one.
    1. Caption

   I created a text file with a number of predefined generic captions that
   could go with any picture. These were either quotes about NYC, broad
   questions, or just basic praise. Some of these included:
Who can name this spot?
Tell us your favorite bar in NYC in the comments!
"You haven't lived until you've died in New York"

   For each post, one of my captions was randomly chosen. I have such a
   large list that I’m not worried about them being used too often or
   overlapping. So, for our example, let’s pick the first one — “Who can
   name this spot?”.

   2. Credit

   This was one of the harder tasks — automatically crediting the source.
   What was particularly tricky was that the Instagram page that the media
   came from wasn’t necessarily the right person to credit. Often, that
   account was also re-sharing the content and crediting the owner in
   their caption or tagging them in the photo.

   I decided I would credit the page where it came from no matter what. I
   would then add more credits if I could possibly decipher the original
   owner as well. I felt I would cover all of my bases this way.

   Let’s take a look at this post by @likenewyorkcity on Instagram. We can
   see that even though he or she was the one who shared it, the real
   owner is @geoffrey.parry who is tagged in the picture and mentioned in
   the caption.

   Ideally, I would like my code to be able to look at this picture and
   return:
Credit: @likenewyorkcity/@geoffrey.parry

   The first part of that is easy; just inputting which account it came
   from. The second part was a little more challenging.

   I used REGEX to look for a number of keywords such as “by” or “photo:”
   and then look for the “@” symbol that followed right after. From there,
   I grabbed the username and believed that to be the second part of my
   credit.

   If none of those keywords existed in the caption, I checked if there
   was anyone tagged in the picture. If there was, I figured they deserved
   the credit. I understand this is an imperfect method, but more times
   than not that’s why someone was tagged and it was a risk worth taking.

   I very often capture exactly the right credit. In fact, many times I’ve
   had people comment on my pictures and say “thank you for sharing!”
   (I’ve added an example of that below).

   3. Hashtags

   Instagram allows you to add 30 hashtags to your picture which will then
   be displayed on that hashtag’s feed. I created a file with over 100
   related hashes:
#NYC #NY #NewYorkCity ... #ThePlaza #NYCInstagram #NYYankees

   and randomly chose 30 to add each time. I did that so after a while, I
   can compare which hashtags lead me to a greater number of likes.

   4. Final Template

   After the three steps were said and done, I was able to fill in my
   template and have a caption that can go along with any post.
Who can name this spot?
.
.
.
Credit: @likenewyorkcity/@geoffrey.parry
.
.
.
#newyorkig #city_of_newyork #ig_newyork#newyorkgram #nycgo #nybucketlist#nyclive
s #nypostnyc #streetsofnyc#winterinnewyork #downtownnyc#brooklynheights #newyork
_photoshoots#newyork_originals #nyloveyou#nycityworld #newyorkbound#newyorkminut
e #imagesofnyc#travelnyc #nyc_exporers #nycbuildings#oneworldtradecenter #flatir
onbuilding#grandcentralterminal #newyorkknights #bigapplenyc #newyorknewyork#man
hattanbridge #brooklynbridge

   Here’s an example of one of my final products:

   I used a generic caption that could go with any picture of NYC. I
   credited both the Instagram account it came from and the original
   source. If you look at the comments, you can see the original owner
   thanking me for sharing. And I added thirty hashtags to boost my post
   as well.

Posting

   I now have a central repository of relevant media and a process of
   generating a caption for each of these posts. Now, it’s time to do just
   that — post.

   I spun up an EC2 instance on AWS to host my code. I chose this route
   because it’s more reliable than my personal machine — it’s always on
   and connected to the internet and I knew it would all fit under the
   free-tier limits.

   I wrote a Python script that randomly grabs one of these pictures and
   auto-generates a caption after the scraping and cleaning process is
   completed. Using my API, I was able to write code that does the actual
   posting for me. I scheduled a cron-job to run around 8:00 AM, 2:00 PM,
   and 7:30 PM every day.

   At this point, I’ve completely automated the content finding and
   posting process. I no longer have to worry about finding media and
   posting every day, it’s being done for me.

Growing My Following

   It isn’t enough to just post — I need to enact some methodologies to
   grow my following as well. And since I won’t ever be on the account
   myself doing any of this manually, I’ll need to automate that too. The
   idea was to get my account exposed to an interested audience by
   interacting directly with those people.

   The interaction script that I wrote runs from 10:00 AM to 7:00 PM EST,
   the time range that I believed Instagram to be most active. Throughout
   the day, my account methodically follows, unfollows, and likes relevant
   users and photos in order to have the same be done back to me.

   Following (More Data Science)

   If you use Instagram, I’m sure you’ve been part of this before whether
   you realize it or not. This method is very common for accounts that are
   trying to increase their following. One day you follow an interesting
   Instagram page in the fitness niche, and the next day you’re being
   followed by a bunch of bodybuilders and fitness models. This seems
   extremely trivial, and it is, but it’s very effective.

   The issue here is that you can’t just follow willy-nilly on Instagram.
   Their algorithm is very very strict, so they will cut you off or even
   ban your account if you go overboard and follow too many accounts in
   one day. Additionally, you can be following at most 7,500 users at one
   time on Instagram. After a lot of testing, I’ve found you can get away
   with following 400 people and unfollowing 400 people in a single day.
   Therefore, each follow is extremely precious. You don’t want to waste a
   follow on someone who is unlikely to follow you back because you only
   have so many users that you can follow in one day. I decided to capture
   the metadata of my activity and make a model to predict how likely
   someone would be to follow you back, so I wouldn’t waste a precious
   follow on someone who was unlikely to return the favor.

   I spent a few minutes manually gathering 20+ bigger accounts in the
   same niche as me. I had no initial data, so the first few weeks would
   be me randomly performing these actions to grow my following, but more
   importantly, I needed to capture as much metadata as possible so I can
   make my model.

   I cycled through these 20+ related accounts and followed the users who
   followed them, liked their pictures, or commented on their posts. With
   each follow I captured as much metadata as possible about the user into
   a CSV file. Some of this metadata included their follower/following
   ratio, if they were public or private, or if they had a profile picture
   or not.

   Every day, my script would go through this CSV and label the missing
   response variable, which is if they followed back or not. I gave each
   user two full days before labeling him or her 0, 1, or 2 — 2 being the
   most desirable outcome. 0 indicated that the user did not follow back,
   1 indicated that they followed back but didn’t interact with me in my
   last ten pictures (liking or commenting), and 2 indicated if they
   followed back AND interacted on one of my last ten posts. My dataset
   looked something like this:
Response Variable:
Follow back (0,1,2)Explanatory Variables:
Came from (The account in which this user was scraped from)
Method (whether they were a liker/commenter/follower of the above)
Timestamp
Private vs public
Missing profile picture
Is business profile
Following/follower ratio
Profile biography
Media count
Gender (predicted from name using a third-party package)

   Before running this data through a ML model, I did some exploratory
   data analysis and found that:
     * Likers and commenters were less likely to follow me back than
       followers, but they were more likely to engage with me. This tells
       me although they are less abundant in quantity, they are higher in
       quality.
     * Following people in the morning resulted in a higher follow-back
       rate than in the nighttime.
     * Public accounts are much more likely to follow me back than private
       accounts.
     * Females were more likely to follow back my NYC-based account than
       were males.
     * Those that were following more people than they had following them
       (following/follower ratio > 1.0) were more likely to follow me
       back.

   From just the above insights, I was able to refine my initial search of
   users. I adjusted my settings to only follow in the morning and to look
   primarily for females. Now I was finally able to make a machine
   learning model to predict the likelihood of a follow back based on a
   user’s metadata before interacting with them. This allows me to not
   waste one of my already limited daily follows on someone who has a very
   small chance of following me back.

   I chose to use the Random Forest algorithm to classify the follow back
   outcome. I originally was using a number of different decision trees
   before I had a set structure or outcome variable because I wanted to
   see the visual flowcharts that come along with them. The Random Forest
   is an enhancement of the decision tree that provides a number of tweaks
   to correct many of the inconsistencies in the individual trees. I was
   consistently seeing an accuracy of over 80% on my test data after
   modeling to my training data, so it was an effective model for me. I
   implemented this in my code on my scraped users to optimize follow
   usage and saw tremendous growth in my following.

   Unfollowing

   After two days, I would unfollow the people I had followed. This gave
   me enough time to capture if they would follow me back or not. This
   allowed me to collect data to continue to grow.

   You have to unfollow the people you follow for two reasons. The first
   is that you cannot be following over 7,500 people at any time. The
   second is because — although artificial — you want to have your
   follower/following ratio as high as possible as it is a sign of a more
   desirable account.

   This is an easy task because there aren’t any decisions that need to be
   made. You follow 400 people in a day, and two days later you unfollow
   those exact people.

   Liking

   Liking can also supplement your account. I didn’t put nearly as much
   effort in choosing the pictures to like as liking isn’t proven to give
   you that much of gain in followers compared to the following method
   described above. I simply gave a predefined set of hashtags, looped
   through their feeds, and liked the pictures in hopes those users would
   return the favor.

Auto-Sales

   At this point, I have a complete self-sustaining robotic Instagram. My
   NYC page, on its own, is finding relevant content, weeding out bad
   potential posts, generating credits and a caption, and posting
   throughout the day. In addition, from 7:00 AM to 10:00 PM, it is
   growing its presence by automatically liking, following, and
   unfollowing with an intrigued audience which has been further redefined
   by some data science algorithms. The best part is that it seems more
   human than most accounts in the same niche.

   For a month or two, I sat back and watched my product grow. I would see
   an increase of anywhere between 100 and 500 followers a day all the
   while enjoying some beautiful pictures of the city I love.

   I was able to go about my life; work at my job, go out with friends,
   see a movie— never having to worry about spending any time manually
   growing my page. It had the formula to do its thing while I did my
   thing.

   Once I had 20,000 followers I decided it was time to use this page to
   get some free meals. Again, I automated my sales pitch too.

   I made a direct message template which I tried to keep as generic as
   possible. I wanted to be able to use the same message whether it was a
   restaurant, a theatre, a museum, or a store. This is what I came up
   with:
Hello {ACCOUNT NAME}
My name is Chris and I run this Instagram account! We have over {FOLLOWER COUNT}
 followers in the New York City area, and many of them comment on my posts or me
ssage me about the best restaurants to eat at, bars to go to, or attractions to
visit.
We would love to make some sort of deal with you to sponsor your establishment.
We would post your place, the address, tag your page, and recommend everyone go
check it out. It will stay up forever, and you can even write the caption and ma
ke the post to send to me if you want. We will also post you on our instagram st
ory with a swipe-up link to your website. In exchange, I would ask for a free ex
perience, small gift card, discount, or coupon to your place.
If you have any interest, please message me back or send me an email!
Thanks!
Chris

   Here, I just need to impute the account name and the number of
   followers I have at the time of the message.

   My goal was to find business Instagrams and give them my pitch. A
   business profile is slightly different from a normal one — it allows
   the user to add their email, phone number, directions, and other
   buttons on their page. But most importantly, they have a category label
   right on their profile.

   The above is an example of a business profile. Right under the name in
   the top left, it says “Korean Restaurant” and it has call-to-action
   buttons such as call, email, and directions at the top.

   I wrote a Python script that looks for these pages and automatically
   sends them a message from my account. The script takes two parameters,
   a starting hashtag, and a string to look for in the category label. In
   my case, I used the hashtag “Manhattan” and the string “restaurant”.

   What this script does is it goes to the hashtag feed and loads a bunch
   of photos. It then loops through the posts until it hits one that has
   users tagged in the photo. If it does, it goes into the tags and checks
   if they are a business page. If it is, it looks at the category. If the
   category includes the word “restaurant”, it sends them my message. The
   nice part about business profiles is that they often have emails on
   their page. If they do, I automatically send them an email follow-up to
   my Instagram direct message. I can change the hashtag to something like
   #TimesSquare and I can change the string to something like “museum” if
   my goals change down the road.

   If I go into my account, I will see the message that it auto-generated
   and sent.

   And if I go to my Gmail outbox, I’ll see:

   Finally, I have a script that monitors my inbox for any responses and
   alerts me if so. If there is a response, I finally do some manual work
   and negotiate with my potential client.

   Along with the posting process and the growth process, this runs
   throughout the day without the need for any human manipulation.

The Results

   The results are better than you might initially imagine. I have
   restaurants basically throwing gift cards and free meals my way in
   exchange for an Instagram promotion.

   Due to the power of AI, automation, and data science — I am able to sit
   back and relax while my code does the work for me. It acts as a source
   of entertainment while at the same time being my salesman.

   I hope this helps inspire some creativity when it comes to social
   media. Anyone can use these methods whether they are technical enough
   to automate or if they need to do it by hand. Instagram is a powerful
   tool and can be used for a variety of business benefits.

   Feel free to reach out with any questions!

   crb4595@gmail.com

   www.linkedin.com/in/chris-buetti/

   twitter: @ChrisBuetti

   www.digitalprezence.com

(BUTTON) 6.8K

     * Social Media
     * Python
     * Instagram
     * Data Science
     * Automation

(BUTTON) 6.8K claps

   (BUTTON)
   (BUTTON)
   Chris Buetti

   Written by

Chris Buetti

   (BUTTON) Follow

Data Engineer at NBA, B.S. Mathematical Statistics Wake Forest University
2017, New York, NY

   (BUTTON) Follow
   See responses (35)

Discover Medium

   Welcome to a place where words matter. On Medium, smart voices and
   original ideas take center stage - with no ads in sight. Watch

Make Medium yours

   Follow all the topics you care about, and we’ll deliver the best
   stories for you to your homepage and inbox. Explore

Become a member

   Get unlimited access to the best stories on Medium — and support
   writers while you’re at it. Just $5/month. Upgrade
   AboutHelpLegal
   #publisher Medium alternate

   Become a member
   Sign in
   (BUTTON) Get started

Something is wrong on the internet

   James Bridle
   James Bridle
   (BUTTON) Follow
   Nov 6, 2017 · 21 min read

   I’m James Bridle. I’m a writer and artist concerned with technology and
   culture. I usually write on my own blog, but frankly I don’t want what
   I’m talking about here anywhere near my own site. Please be advised:
   this essay describes disturbing things and links to disturbing graphic
   and video content. You don’t have to read it, and are advised to take
   caution exploring further.

   As someone who grew up on the internet, I credit it as one of the most
   important influences on who I am today. I had a computer with internet
   access in my bedroom from the age of 13. It gave me access to a lot of
   things which were totally inappropriate for a young teenager, but it
   was OK. The culture, politics, and interpersonal relationships which I
   consider to be central to my identity were shaped by the internet, in
   ways that I have always considered to be beneficial to me personally. I
   have always been a critical proponent of the internet and everything it
   has brought, and broadly considered it to be emancipatory and
   beneficial. I state this at the outset because thinking through the
   implications of the problem I am going to describe troubles my own
   assumptions and prejudices in significant ways.

   One of the thus-far hypothetical questions I ask myself frequently is
   how I would feel about my own children having the same kind of access
   to the internet today. And I find the question increasingly difficult
   to answer. I understand that this is a natural evolution of attitudes
   which happens with age, and at some point this question might be a lot
   less hypothetical. I don’t want to be a hypocrite about it. I would
   want my kids to have the same opportunities to explore and grow and
   express themselves as I did. I would like them to have that choice. And
   this belief broadens into attitudes about the role of the internet in
   public life as whole.

   I’ve also been aware for some time of the increasingly symbiotic
   relationship between younger children and YouTube. I see kids engrossed
   in screens all the time, in pushchairs and in restaurants, and there’s
   always a bit of a Luddite twinge there, but I am not a parent, and I’m
   not making parental judgments for or on anyone else. I’ve seen family
   members and friend’s children plugged into Peppa Pig and nursery rhyme
   videos, and it makes them happy and gives everyone a break, so OK.

   But I don’t even have kids and right now I just want to burn the whole
   thing down.

   Someone or something or some combination of people and things is using
   YouTube to systematically frighten, traumatise, and abuse children,
   automatically and at scale, and it forces me to question my own beliefs
   about the internet, at every level. Much of what I am going to describe
   next has been covered elsewhere, although none of the mainstream
   coverage I’ve seen has really grasped the implications of what seems to
   be occurring.

   To begin: Kid’s YouTube is definitely and markedly weird. I’ve been
   aware of its weirdness for some time. Last year, there were a number of
   articles posted about the Surprise Egg craze. Surprise Eggs videos
   depict, often at excruciating length, the process of unwrapping Kinder
   and other egg toys. That’s it, but kids are captivated by them. There
   are thousands and thousands of these videos and thousands and
   thousands, if not millions, of children watching them.

   IFRAME:
   https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtub
   e.com%2Fembed%2FeLYFUZDgspU%3Ffeature%3Doembed&url=http%3A%2F%2Fwww.you
   tube.com%2Fwatch%3Fv%3DeLYFUZDgspU&image=https%3A%2F%2Fi.ytimg.com%2Fvi
   %2FeLYFUZDgspU%2Fhqdefault.jpg&key=a19fcc184b9711e1b4764040d3dc5c07&typ
   e=text%2Fhtml&schema=youtube

   From the article linked above:

     The maker of my particular favorite videos is “Blu Toys Surprise
     Brinquedos & Juegos,” and since 2010 he seems to have accrued 3.7
     million subscribers and just under 6 billion views for a
     kid-friendly channel entirely devoted to opening surprise eggs and
     unboxing toys. The video titles are a continuous pattern of obscure
     branded lines and tie-ins: “Surprise Play Doh Eggs Peppa Pig Stamper
     Cars Pocoyo Minecraft Smurfs Kinder Play Doh Sparkle Brilho,” “Cars
     Screamin’ Banshee Eats Lightning McQueen Disney Pixar,” “Disney Baby
     Pop Up Pals Easter Eggs SURPRISE.”

     As I write this he has done a total of 4,426 videos and counting.
     With so many views — for comparison, Justin Bieber’s official
     channel has more than 10 billion views, while full-time YouTube
     celebrity PewDiePie has nearly 12 billion — it’s likely this man
     makes a living as a pair of gently murmuring hands that unwrap
     Kinder eggs. (Surprise-egg videos are all accompanied by pre-roll,
     and sometimes mid-video and ads.)

   That should give you some idea of just how odd the world of kids online
   video is, and that list of video titles hints at the extraordinary
   range and complexity of this situation. We’ll get into the latter in a
   minute; for the moment know that it’s already very strange, if
   apparently pretty harmless, out there.

   Another huge trope, especially the youngest children, is nursery rhyme
   videos.

   IFRAME:
   https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtub
   e.com%2Fembed%2FBHcFQ9gaMF4%3Ffeature%3Doembed&url=http%3A%2F%2Fwww.you
   tube.com%2Fwatch%3Fv%3DBHcFQ9gaMF4&image=https%3A%2F%2Fi.ytimg.com%2Fvi
   %2FBHcFQ9gaMF4%2Fhqdefault.jpg&key=a19fcc184b9711e1b4764040d3dc5c07&typ
   e=text%2Fhtml&schema=youtube

   Little Baby Bum, which made the above video, is the 7th most popular
   channel on YouTube. With just 515 videos, they have accrued 11.5
   million subscribers and 13 billion views. Again, there are questions as
   to the accuracy of these numbers, which I’ll get into shortly, but the
   key point is that this is a huge, huge network and industry.

   On-demand video is catnip to both parents and to children, and thus to
   content creators and advertisers. Small children are mesmerised by
   these videos, whether it’s familiar characters and songs, or simply
   bright colours and soothing sounds. The length of many of these videos
   — one common video tactic is to assemble many nursery rhyme or cartoon
   episodes into hour+ compilations —and the way that length is marketed
   as part of the video’s appeal, points to the amount of time some kids
   are spending with them.

   YouTube broadcasters have thus developed a huge number of tactics to
   draw parents’ and childrens’ attention to their videos, and the
   advertising revenues that accompany them. The first of these tactics is
   simply to copy and pirate other content. A simple search for “Peppa
   Pig” on YouTube in my case yielded “About 10,400,000 results” and the
   front page is almost entirely from the verified “Peppa Pig Official
   Channel”, while one is from an unverified channel called Play Go Toys,
   which you really wouldn’t notice unless you were looking out for it:

   Play Go Toys’ channel consists of (I guess?) pirated Peppa Pig and
   other cartoons, videos of toy unboxings (another kid magnet), and
   videos of, one supposes, the channel owner’s own children. I am not
   alleging anything bad about Play Go Toys; I am simply illustrating how
   the structure of YouTube facilitates the delamination of content and
   author, and how this impacts on our awareness and trust of its source.

   As another blogger notes, one of the traditional roles of branded
   content is that it is a trusted source. Whether it’s Peppa Pig on
   children’s TV or a Disney movie, whatever one’s feelings about the
   industrial model of entertainment production, they are carefully
   produced and monitored so that kids are essentially safe watching them,
   and can be trusted as such. This no longer applies when brand and
   content are disassociated by the platform, and so known and trusted
   content provides a seamless gateway to unverified and potentially
   harmful content.

   (Yes, this is the exact same process as the delamination of trusted
   news media on Facebook feeds and in Google results that is currently
   wreaking such havoc on our cognitive and political systems and I am not
   going to explicitly explore that relationship further here, but it is
   obviously deeply significant.)

   A second way of increasing hits on videos is through keyword/hashtag
   association, which is a whole dark art unto itself. When some trend,
   such as Surprise Egg videos, reaches critical mass, content producers
   pile onto it, creating thousands and thousands more of these videos in
   every possible iteration. This is the origin of all the weird names in
   the list above: branded content and nursery rhyme titles and “surprise
   egg” all stuffed into the same word salad to capture search results,
   sidebar placement, and “up next” autoplay rankings.

   IFRAME:
   https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtub
   e.com%2Fembed%2F3xqqj9o7TgA%3Ffeature%3Doembed&url=http%3A%2F%2Fwww.you
   tube.com%2Fwatch%3Fv%3D3xqqj9o7TgA&image=https%3A%2F%2Fi.ytimg.com%2Fvi
   %2F3xqqj9o7TgA%2Fhqdefault.jpg&key=a19fcc184b9711e1b4764040d3dc5c07&typ
   e=text%2Fhtml&schema=youtube

   A striking example of the weirdness is the Finger Family videos
   (harmless example embedded above). I have no idea where they came from
   or the origin of the children’s rhyme at the core of the trope, but
   there are at least 17 million versions of this currently on YouTube,
   and again they cover every possible genre, with billions and billions
   of aggregated views.

   Once again, the view numbers of these videos must be taken under
   serious advisement. A huge number of these videos are essentially
   created by bots and viewed by bots, and even commented on by bots. That
   is a whole strange world in and of itself. But it shouldn’t obscure
   that there are also many actual children, plugged into iphones and
   tablets, watching these over and over again — in part accounting for
   the inflated view numbers — learning to type basic search terms into
   the browser, or simply mashing the sidebar to bring up another video.

   What I find somewhat disturbing about the proliferation of even
   (relatively) normal kids videos is the impossibility of determining the
   degree of automation which is at work here; how to parse out the gap
   between human and machine. The example above, from a channel called
   Bounce Patrol Kids, with almost two million subscribers, show this
   effect in action. It posts professionally produced videos, with
   dedicated human actors, at the rate of about one per week. Once again,
   I am not alleging anything untoward about Bounce Patrol, which clearly
   follows in the footsteps of pre-digital kid sensations like their
   fellow Australians The Wiggles.

   And yet, there is something weird about a group of people endlessly
   acting out the implications of a combination of algorithmically
   generated keywords: “Halloween Finger Family & more Halloween Songs for
   Children | Kids Halloween Songs Collection”, “Australian Animals Finger
   Family Song | Finger Family Nursery Rhymes”, “Farm Animals Finger
   Family and more Animals Songs | Finger Family Collection - Learn
   Animals Sounds”, “Safari Animals Finger Family Song | Elephant, Lion,
   Giraffe, Zebra & Hippo! Wild Animals for kids”, “Superheroes Finger
   Family and more Finger Family Songs! Superhero Finger Family
   Collection”, “Batman Finger Family Song — Superheroes and Villains!
   Batman, Joker, Riddler, Catwoman” and on and on and on. This is content
   production in the age of algorithmic discovery — even if you’re a
   human, you have to end up impersonating the machine.

   Other channels do away with the human actors to create infinite
   reconfigurable versions of the same videos over and over again. What is
   occurring here is clearly automated. Stock animations, audio tracks,
   and lists of keywords being assembled in their thousands to produce an
   endless stream of videos. The above channel, Videogyan 3D Rhymes —
   Nursery Rhymes & Baby Songs, posts several videos a week, in
   increasingly byzantine combinations of keywords. They have almost five
   million subscribers — more than double Bounce Patrol — although once
   again it’s impossible to know who or what is actually racking up these
   millions and millions of views.

   I’m trying not to turn this essay into an endless list of examples, but
   it’s important to grasp how vast this system is, and how indeterminate
   its actions, process, and audience. It’s also international: there are
   variations of Finger Family and Learn Colours videos for Tamil epics
   and Malaysian cartoons which are unlikely to pop up in any Anglophone
   search results. This very indeterminacy and reach is key to its
   existence, and its implications. Its dimensionality makes it difficult
   to grasp, or even to really think about.

   We’ve encountered pretty clear examples of the disturbing outcomes of
   full automation before — some of which have been thankfully leavened
   with a dark kind of humour, others not so much. Much has been made of
   the algorithmic interbreeding of stock photo libraries and on-demand
   production of everything from tshirts to coffee mugs to infant onesies
   and cell phone covers. The above example, available until recently on
   Amazon, is one such case, and the story of how it came to occur is
   fascinating and weird but essentially comprehensible. Nobody set out to
   create phone cases with drugs and medical equipment on them, it was
   just a deeply weird mathematical/probabilistic outcome. The fact that
   it took a while to notice might ring some alarm bells however.

   Likewise, the case of the “Keep Calm and Rape A Lot” tshirts (along
   with the “Keep Calm and Knife Her” and “Keep Calm and Hit Her” ones) is
   depressing and distressing but comprehensible. Nobody set out to create
   these shirts: they just paired an unchecked list of verbs and pronouns
   with an online image generator. It’s quite possible that none of these
   shirts ever physically existed, were ever purchased or worn, and thus
   that no harm was done. Once again though, the people creating this
   content failed to notice, and neither did the distributor. They
   literally had no idea what they were doing.

   What I will argue, on the basis of these cases and of those I’m going
   to describe further, is that the scale and logic of the system is
   complicit in these outputs, and requires us to think through their
   implications.

   (Also again: I’m not going to dig into the wider social implications of
   such processes outside the scope of what I am writing about here, but
   it’s clear that one can draw a clear line from examples such as these
   to pressing contemporary issues such as racial and gender bias in big
   data and machine intelligence-driven systems, which require urgent
   attention but in the same manner do not have anything resembling easy
   or even preferable solutions.)

   Let’s look at just one video among the piles of kid videos, and try to
   parse out where it comes from. It’s important to stress that I didn’t
   set out to find this particular video: it appeared organically and
   highly ranked in a search for ‘finger family’ in an incognito browser
   window (i.e. it should not have been influenced by previous searches).
   This automation takes us to very, very strange places, and at this
   point the rabbithole is so deep that it’s impossible to know how such a
   thing came into being.

   Once again, a content warning: this video is not inappropriate in any
   way, but it is decidedly off, and contains elements which might trouble
   anyone. It’s very mild on the scale of such things, but. I describe it
   below if you don’t want to watch it and head down that road. This
   warning will recur.

   IFRAME:
   https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtub
   e.com%2Fembed%2FD52hg9ogvWc%3Ffeature%3Doembed&url=http%3A%2F%2Fwww.you
   tube.com%2Fwatch%3Fv%3DD52hg9ogvWc&image=https%3A%2F%2Fi.ytimg.com%2Fvi
   %2FD52hg9ogvWc%2Fhqdefault.jpg&key=a19fcc184b9711e1b4764040d3dc5c07&typ
   e=text%2Fhtml&schema=youtube

   The above video is entitled Wrong Heads Disney Wrong Ears Wrong Legs
   Kids Learn Colors Finger Family 2017 Nursery Rhymes. The title alone
   confirms its automated provenance. I have no idea where the “Wrong
   Heads” trope originates, but I can imagine, as with the Finger Family
   Song, that somewhere there is a totally original and harmless version
   that made enough kids laugh that it started to climb the algorithmic
   rankings until it made it onto the word salad lists, combining with
   Learn Colors, Finger Family, and Nursery Rhymes, and all of these
   tropes — not merely as words but as images, processes, and actions — to
   be mixed into what we see here.

   The video consists of a regular version of the Finger Family song
   played over an animation of character heads and bodies from Disney’s
   Aladdin swapping and intersecting. Again, this is weird but frankly no
   more than the Surprise Egg videos or anything else kids watch. I get
   how innocent it is. The offness creeps in with the appearance of a
   non-Aladdin character —Agnes, the little girl from Despicable Me. Agnes
   is the arbiter of the scene: when the heads don’t match up, she cries,
   when they do, she cheers.

   The video’s creator, BABYFUN TV (screenshot above), has produced many
   similar videos. As many of the Wrong Heads videos as I could bear to
   watch all work in exactly the same way. The character Hope from Inside
   Out weeps through a Smurfs and Trolls head swap. It goes on and on. I
   get the game, but the constant overlaying and intermixing of different
   tropes starts to get inside you. BABYFUN TV only has 170 subscribers
   and very low view rates, but then there are thousands and thousands of
   channels like this. Numbers in the long tail aren’t significant in the
   abstract, but in their accumulation.

   The question becomes: how did this come to be? The “Bad Baby” trope
   also present on BABYFUN TV features the same crying. While I find it
   disturbing, I can understand how it might provide some of the rhythm or
   cadence or relation to their own experience that actual babies are
   attracted to in this content, although it has been warped and stretched
   through algorithmic repetition and recombination in ways that I don’t
   think anyone actually wants to happen.
   Screenshot from Toy Freaks channel

   [Edit, 21/11/2017: Following the publication of this article, the Toy
   Freaks channel was removed by YouTube as part of a widespread removal
   of contentious content.]

   Toy Freaks is a hugely popular channel (68th on the platform) which
   features a father and his two daughters playing out — or in some cases
   perhaps originating — many of the tropes we’ve identified so far,
   including “Bad Baby”, (previously embedded above). As well as nursery
   rhymes and learning colours, Toy Freaks specialises in gross-out
   situations, as well as activities which many, many viewers feel border
   on abuse and exploitation, if not cross the line entirely, including
   videos of the children vomiting and in pain. Toy Freaks is a YouTube
   verified channel, whatever that means. (I think we know by now it means
   nothing useful.)

   As with Bounce Patrol Kids, however you feel about the content of these
   videos, it feels impossible to know where the automation starts and
   ends, who is coming up with the ideas and who is roleplaying them. In
   turn, the amplification of tropes in popular, human-led channels such
   as Toy Freaks leads to them being endlessly repeated across the network
   in increasingly outlandish and distorted recombinations.

   There’s a second level of what I’m characterising as human-led videos
   which are much more disturbing than the mostly distasteful activities
   of Toy Freaks and their kin. Here is a relatively mild, but still
   upsetting example:

   A step beyond the simply pirated Peppa Pig videos mentioned previously
   are the knock-offs. These too seem to teem with violence. In the
   official Peppa Pig videos, Peppa does indeed go to the dentist, and the
   episode in which she does so seems to be popular — although,
   confusingly, what appears to be the real episode is only available on
   an unofficial channel. In the official timeline, Peppa is appropriately
   reassured by a kindly dentist. In the version above, she is basically
   tortured, before turning into a series of Iron Man robots and
   performing the Learn Colours dance. A search for “peppa pig dentist”
   returns the above video on the front page, and it only gets worse from
   here.

   [Edit, 21/11/2017: the original video cited here has now been removed
   as part of YouTube’s recent purge, although many similar videos remain
   on the platform.]

   Disturbing Peppa Pig videos, which tend towards extreme violence and
   fear, with Peppa eating her father or drinking bleach, are, it turns
   out very widespread. They make up an entire YouTube subculture. Many
   are obviously parodies, or even satires of themselves, in the pretty
   common style of the internet’s outrageous, deliberately offensive kind.
   All the 4chan tropes are there, the trolls are out, we know this.

   In the example above, the agency is less clear: the video starts with a
   trollish Peppa parody, but later syncs into the kind of automated
   repetition of tropes we’ve seen already. I don’t know which camp it
   belongs to. Maybe it’s just trolls. I kind of hope it is. But I don’t
   think so. Trolls don’t cover the intersection of human actors and more
   automated examples further down the line. They’re at play here, but
   they’re not the whole story.

   I suppose it’s naive not to see the deliberate versions of this coming,
   but many are so close to the original, and so unsignposted — like the
   dentist example — that many, many kids are watching them. I understand
   that most of them are not trying to mess kids up, not really, even
   though they are.

   I’m trying to understand why, as plainly and simply troubling as it is,
   this is not a simple matter of “won’t somebody think of the children”
   hand-wringing. Obviously this content is inappropriate, obviously there
   are bad actors out there, obviously some of these videos should be
   removed. Obviously too this raises questions of fair use,
   appropriation, free speech and so on. But reports which simply
   understand the problem through this lens fail to fully grasp the
   mechanisms being deployed, and thus are incapable of thinking its
   implications in totality, and responding accordingly.

   The New York Times, headlining their article on a subset of this issue
   “On YouTube Kids, Startling Videos Slip Past Filters”, highlights the
   use of knock-off characters and nursery rhymes in disturbing content,
   and frames it as a problem of moderation and legislation. YouTube Kids,
   an official app which claims to be kid-safe but is quite obviously not,
   is the problem identified, because it wrongly engenders trust in users.
   An article in the British tabloid The Sun, “Kids left traumatised after
   sick YouTube clips showing Peppa Pig characters with knives and guns
   appear on app for children” takes the same line, with an added dose of
   right-wing technophobia and self-righteousness. But both stories take
   at face value YouTube’s assertions that these results are incredibly
   rare and quickly removed: assertions utterly refuted by the
   proliferation of the stories themselves, and the growing number of
   social media posts, largely by concerned parents, from which they
   arise.

   But as with Toy Freaks, what is concerning to me about the Peppa videos
   is how the obvious parodies and even the shadier knock-offs interact
   with the legions of algorithmic content producers until it is
   completely impossible to know what is going on. (“The creatures outside
   looked from pig to man, and from man to pig, and from pig to man again;
   but already it was impossible to say which was which.”)
   Good Baby Toys channel

   Here’s what is basically a version of Toy Freaks produced in Asia
   (screenshot above). Here’s one from Russia. I don’t really want to use
   the term “human-led” any more about these videos, although they contain
   all the same tropes and actual people acting them out. I no longer have
   any idea what’s going on here and I really don’t want to and I’m
   starting to think that that is kind of the point. That’s part of why
   I’m starting to think about the deliberateness of this all. There is a
   lot of effort going into making these. More than spam revenue can
   generate — can it? Who’s writing these scripts, editing these videos?
   Once again, I want to stress: this is still really mild, even funny
   stuff compared to a lot of what is out there.

   Here are a few things which are disturbing me:

   The first is the level of horror and violence on display. Some of the
   times it’s troll-y gross-out stuff; most of the time it seems deeper,
   and more unconscious than that. The internet has a way of amplifying
   and enabling many of our latent desires; in fact, it’s what it seems to
   do best. I spend a lot of time arguing for this tendency, with regards
   to human sexual freedom, individual identity, and other issues. Here,
   and overwhelmingly it sometimes feels, that tendency is itself a
   violent and destructive one.

   The second is the levels of exploitation, not of children because they
   are children but of children because they are powerless. Automated
   reward systems like YouTube algorithms necessitate exploitation in the
   same way that capitalism necessitates exploitation, and if you’re
   someone who bristles at the second half of that equation then maybe
   this should be what convinces you of its truth. Exploitation is encoded
   into the systems we are building, making it harder to see, harder to
   think and explain, harder to counter and defend against. Not in a
   future of AI overlords and robots in the factories, but right here,
   now, on your screen, in your living room and in your pocket.

   Many of these latest examples confound any attempt to argue that nobody
   is actually watching these videos, that these are all bots. There are
   humans in the loop here, even if only on the production side, and I’m
   pretty worried about them too.

   I’ve written enough, too much, but I feel like I actually need to
   justify all this raving about violence and abuse and automated systems
   with an example that sums it up. Maybe after everything I’ve said you
   won’t think it’s so bad. I don’t know what to think any more.

   [Edit, 21/11/2017: the original video cited here has now been removed
   as part of YouTube’s recent purge, although many similar videos remain
   on the platform. The video used animations from the Grand Theft Auto
   game series overlaid with cartoon characters assaulting, killing, and
   burying one another.]

   This video, BURIED ALIVE Outdoor Playground Finger Family Song Nursery
   Rhymes Animation Education Learning Video, contains all of the elements
   we’ve covered above, and takes them to another level. Familiar
   characters, nursery tropes, keyword salad, full automation, violence,
   and the very stuff of kids’ worst dreams. And of course there are vast,
   vast numbers of these videos. Channel after channel after channel of
   similar content, churned out at the rate of hundreds of new videos
   every week. Industrialised nightmare production.

   For the final time: There is more violent and more sexual content like
   this available. I’m not going to link to it. I don’t believe in
   traumatising other people, but it’s necessary to keep stressing it, and
   not dismiss the psychological effect on children of things which aren’t
   overtly disturbing to adults, just incredibly dark and weird.

   A friend who works in digital video described to me what it would take
   to make something like this: a small studio of people (half a dozen,
   maybe more) making high volumes of low quality content to reap ad
   revenue by tripping certain requirements of the system (length in
   particular seems to be a factor). According to my friend, online kids’
   content is one of the few alternative ways of making money from 3D
   animation because the aesthetic standards are lower and independent
   production can profit through scale. It uses existing and easily
   available content (such as character models and motion-capture
   libraries) and it can be repeated and revised endlessly and mostly
   meaninglessly because the algorithms don’t discriminate — and neither
   do the kids.

   These videos, wherever they are made, however they come to be made, and
   whatever their conscious intention (i.e. to accumulate ad revenue) are
   feeding upon a system which was consciously intended to show videos to
   children for profit. The unconsciously-generated, emergent outcomes of
   that are all over the place.

   To expose children to this content is abuse. We’re not talking about
   the debatable but undoubtedly real effects of film or videogame
   violence on teenagers, or the effects of pornography or extreme images
   on young minds, which were alluded to in my opening description of my
   own teenage internet use. Those are important debates, but they’re not
   what is being discussed here. What we’re talking about is very young
   children, effectively from birth, being deliberately targeted with
   content which will traumatise and disturb them, via networks which are
   extremely vulnerable to exactly this form of abuse. It’s not about
   trolls, but about a kind of violence inherent in the combination of
   digital systems and capitalist incentives. It’s down to that level of
   the metal.

   This, I think, is my point: The system is complicit in the abuse.

   And right now, right here, YouTube and Google are complicit in that
   system. The architecture they have built to extract the maximum revenue
   from online video is being hacked by persons unknown to abuse children,
   perhaps not even deliberately, but at a massive scale. I believe they
   have an absolute responsibility to deal with this, just as they have a
   responsibility to deal with the radicalisation of (mostly) young
   (mostly) men via extremist videos — of any political persuasion. They
   have so far showed absolutely no inclination to do this, which is in
   itself despicable. However, a huge part of my troubled response to this
   issue is that I have no idea how they can respond without shutting down
   the service itself, and most systems which resemble it. We have built a
   world which operates at scale, where human oversight is simply
   impossible, and no manner of inhuman oversight will counter most of the
   examples I’ve used in this essay. The asides I’ve kept in parentheses
   throughout, if expanded upon, would allow one with minimal effort to
   rewrite everything I’ve said, with very little effort, to be not about
   child abuse, but about white nationalism, about violent religious
   ideologies, about fake news, about climate denialism, about 9/11
   conspiracies.

   This is a deeply dark time, in which the structures we have built to
   sustain ourselves are being used against us — all of us — in systematic
   and automated ways. It is hard to keep faith with the network when it
   produces horrors such as these. While it is tempting to dismiss the
   wilder examples as trolling, of which a significant number certainly
   are, that fails to account for the sheer volume of content weighted in
   a particularly grotesque direction. It presents many and complexly
   entangled dangers, including that, just as with the increasing focus on
   alleged Russian interference in social media, such events will be used
   as justification for increased control over the internet, increasing
   censorship, and so on. This is not what many of us want.

   I’m going to stop here, saying only this:

   What concerns me is not just the violence being done to children here,
   although that concerns me deeply. What concerns me is that this is just
   one aspect of a kind of infrastructural violence being done to all of
   us, all of the time, and we’re still struggling to find a way to even
   talk about it, to describe its mechanisms and its actions and its
   effects. As I said at the beginning of this essay: this is being done
   by people and by things and by a combination of things and people.
   Responsibility for its outcomes is impossible to assign but the damage
   is very, very real indeed.
     __________________________________________________________________

   This essay forms one strand of my book New Dark Age (Verso, 2018). You
   can read an extract from the book, and find out more, here.

(BUTTON) 175K

     * YouTube
     * Google
     * Violence
     * Abuse
     * Internet

(BUTTON) 175K claps

   (BUTTON)
   (BUTTON)
   James Bridle

   Written by

James Bridle

   (BUTTON) Follow

Writer and Artist. http://jamesbridle.com

   (BUTTON) Follow
   See responses (563)

Discover Medium

   Welcome to a place where words matter. On Medium, smart voices and
   original ideas take center stage - with no ads in sight. Watch

Make Medium yours

   Follow all the topics you care about, and we’ll deliver the best
   stories for you to your homepage and inbox. Explore

Become a member

   Get unlimited access to the best stories on Medium — and support
   writers while you’re at it. Just $5/month. Upgrade
   AboutHelpLegal
   #publisher Medium alternate

   Basic Income
   Sign in
   (BUTTON) Get started

   Basic Income

   Photo of the Freightliner Inspiration Truck by eMercedesMenz

Self-Driving Trucks Are Going to Hit Us Like a Human-Driven Truck

The imminent need for basic income in recognition of our machine-driven
future

   Scott Santens
   Scott Santens
   (BUTTON) Follow
   May 14, 2015 · 14 min read
     __________________________________________________________________

   Late last year, I took a road trip with my partner from our home in New
   Orleans, Louisiana to Orlando, Florida and as we drove by town after
   town, we got to talking about the potential effects self-driving
   vehicle technology would have not only on truckers themselves, but on
   all the local economies dependent on trucker salaries. Once one starts
   wondering about this kind of one-two punch to America’s gut, one sees
   the prospects aren’t pretty.

   We are facing the decimation of entire small town economies, a
   disruption the likes of which we haven’t seen since the construction of
   the interstate highway system itself bypassed entire towns. If you
   think this may be a bit of hyperbole… let me back up a bit and start
   with this:
   Source: NPR

   This is a map of the most common job in each US state in 2014.

   It should be clear at a glance just how dependent the American economy
   is on truck drivers. According to the American Trucker Association,
   there are 3.5 million professional truck drivers in the US, and an
   additional 5.2 million people employed within the truck-driving
   industry who don’t drive the trucks. That’s 8.7 million
   trucking-related jobs.

   We can’t stop there though, because the incomes received by these 8.2
   million people create the jobs of others. Those 3.5 million truck
   drivers driving all over the country stop regularly to eat, drink,
   rest, and sleep. Entire businesses have been built around serving their
   wants and needs. Think restaurants and motels as just two examples. So
   now we’re talking about millions more whose employment depends on the
   employment of truck drivers. But we still can’t even stop there.

   Those working in these restaurants and motels along truck-driving
   routes are also consumers within their own local economies. Think about
   what a server spends her paycheck and tips on in her own community, and
   what a motel maid spends from her earnings into the same community.
   That spending creates other paychecks in turn. So now we’re not only
   talking about millions more who depend on those who depend on truck
   drivers, but we’re also talking about entire small town communities
   full of people who depend on all of the above in more rural areas. With
   any amount of reduced consumer spending, these local economies will
   shrink.

   One further important detail to consider is that truck drivers are
   well-paid. They provide a middle class income of about $40,000 per
   year. That’s a higher income than just about half (46%) of all tax
   filers, including those of married households. They are also greatly
   comprised by those without college educations. Truck driving is just
   about the last job in the country to provide a solid middle class
   salary without requiring a post-secondary degree. Truckers are
   essentially the last remnant of an increasingly impoverished population
   once gainfully employed in manufacturing before those middle income
   jobs were mostly all shipped overseas.

   If we now step back and look at the big national picture, we are
   potentially looking at well over 10 million American workers and their
   families whose incomes depend entirely or at least partially on the
   incomes of truck drivers, all of whom markedly comprise what is left of
   the American middle class.

   So as long as the outlook for US trucking is rosy, we’re fine, right?

The Short-Term Job Outlook of the American Trucker

   The trucking industry expects to see 21% more truck driving jobs by
   2020. They also expect to see an increasing shortfall in drivers, with
   over 100,000 jobs open and unable to find drivers to fill them. Higher
   demand than supply of truckers also points to higher pay, so for at
   least the next five years, the future is looking great for truck
   drivers. The only thing that could put a damper on this would be if the
   demand for truck drivers were to say… drive off a sharp cliff.

   That cliff is the self-driving truck.

   The technology already exists to enable trucks to drive themselves.
   Google shocked the world when it announced its self-driving car it had
   already driven over 100,000 miles without accident. These cars have
   since driven over 1.7 million miles and have only been involved in 11
   accidents, all caused by humans and not the computers. And this is
   mostly within metropolitan areas.

     “And as you might expect, we see more accidents per mile driven on
     city streets than on freeways; we were hit 8 times in many fewer
     miles of city driving.” — Chris Urmson, director of Google’s
     self-driving car program

   So according to Google’s experience, the greater danger lies within
   cities and not freeways, and driving between cities involves even fewer
   technological barriers than within them. Therefore, it’s probably
   pretty safe to say driverless freeway travel is even closer to our
   future horizon of driverless transportation. How much closer? It has
   already happened.

   On May 6, 2015, the first self-driving truck hit the American road in
   the state of Nevada.

   IFRAME:
   https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtub
   e.com%2Fembed%2FHdSRUG4KTPA%3Ffeature%3Doembed&url=https%3A%2F%2Fwww.yo
   utube.com%2Fwatch%3Fv%3DHdSRUG4KTPA&image=https%3A%2F%2Fi.ytimg.com%2Fv
   i%2FHdSRUG4KTPA%2Fhqdefault.jpg&key=d04bfffea46d4aeda930ec88cc64b87c&ty
   pe=text%2Fhtml&schema=youtube

   Self-driving trucks are no longer the future. They are the present.
   They’re here.

     “AU 010.” License plates are rarely an object of attention, but this
     one’s special — the funky number is the giveaway. That’s why Daimler
     bigwig Wolfgang Bernhard and Nevada governor Brian Sandoval are
     sharing a stage, mugging for the phalanx of cameras, together
     holding the metal rectangle that will, in just a minute, be slapped
     onto the world’s first officially recognized self-driving truck.

   According to Daimler, these trucks will be in a decade-long testing
   phase, racking up over a million miles before being deemed fit for
   adoption, but the technology isn’t even anything all that new. There’s
   no laser-radar or LIDAR like in Google’s self-driving car. It’s just
   ordinary radar and cameras. The hardware itself is already yesterday’s
   news. They’re just the first ones to throw them into a truck and allow
   truckers to sit back and enjoy the ride, while the truck itself does
   all the driving.

   If the truck needs help, it’ll alert the driver. If the driver doesn’t
   respond, it’ll slowly pull over and wait for further instructions. This
   is nothing fancy. This is not a truck version of KITT from Knight
   Rider. This is just an example of a company and a state government
   getting out of the way of technology and letting it do what it was
   built to do — enable us to do more with less. In the case of
   self-driving trucks, one big improvement in particular is fewer
   accidents.

     In 2012 in the US, 330,000 large trucks were involved in crashes
     that killed nearly 4,000 people, most of them in passenger cars.
     About 90 percent of those were caused by driver error.

   That’s like one and a half 9/11s yearly. Human-driven trucks kill
   people.

   Robot trucks will kill far fewer people, if any, because machines don’t
   get tired. Machines don’t get distracted. Machines don’t look at phones
   instead of the road. Machines don’t drink alcohol or do any kind of
   drugs or involve any number of things that somehow contribute to the
   total number of accidents every year involving trucks. For this same
   reasoning, pilots too are bound to be removed from airplanes.

   Humans are dangerous behind the wheel of anything.

   Robot trucks also don’t need salaries — salaries that stand to go up
   because fewer and fewer people want to be truckers. A company can buy a
   fleet of self-driving trucks and never pay another human salary for
   driving. The only costs will be upkeep of the machinery. No more need
   for health insurance either. Self-driving trucks will also never need
   to stop to rest, for any reason. Routes will take less time to
   complete.

   All of this means the replacement of truckers is inevitable. It is not
   a matter of “if”, it’s only a matter of “when.” So the question then
   becomes, how long until millions of truckers are freshly unemployed and
   what happens to them and all the rest of us as a result?

The Long-Term Job Outlook of the American Trucker

   First, let’s look at the potential time horizons for self-driving cars.
   Tesla intends to release a software update next month that will turn on
   “autopilot” mode, immediately allowing all Tesla Model S drivers to be
   driven between “San Francisco and Seattle without the driver doing
   anything”, in Elon Musk’s own words. The cars actually already have the
   technology to even drive from “parking lot to parking lot”, but that
   ability will remain unactivated by software.

   Tesla-driven humans won’t be able to legally let their cars do all the
   driving, but who are we kidding? There will be Teslas driving
   themselves, saving lives in the process, and governments will need to
   catch up to make that driving legal. This process is already here in
   2015. So when will the process end? When will self-driving cars conquer
   our roads?
   Source: Morgan Stanley

   According to Morgan Stanley, complete autonomous capability will be
   here by 2022, followed by massive market penetration by 2026 and the
   cars we know and love today then entirely extinct in another 20 years
   thereafter.

   Granted, this is only one estimate of many and it’s all educated
   guesswork. So here are some other estimates:
     * Navigant Research: “By 2035, sales of autonomous vehicles will
       reach 95.4 million annually, representing 75% of all light-duty
       vehicle sales.”
     * IHS Automotive: “There should be nearly 54 million self-driving
       cars in use globally by 2035.”
     * ABI Research: “Half of new vehicles shipping in North America to
       have driverless, robotic capabilities by 2032.”
     * Nissan: “In 2020 we’re talking more autonomous drive capability.
       It’s going to be an evolutionary process and 2020 will be the first
       year to truly see some of these capabilities start to be introduced
       in the vehicle.”

   Take all of these estimates together, and we’re looking at a window of
   massive disruption starting somewhere between 2020 and 2030.

   There is no turning the wheel in prevention of driving off this cliff
   either. Capitalism itself has the wheel now, and what the market wants,
   the market gets. Competition will make sure of it. Tesla and Google are
   not the only companies looking to develop autonomous vehicles. There
   are others.

   A company named Veeo Systems is developing vehicles as small as
   2-seaters to as large as 70-seat buses, and will be testing them in 30
   US cities by the end of 2016.

     At 25 to 40 percent cheaper, the cost to ride the driverless public
     transit vehicles will be significantly less expensive than
     traditional buses and trains… The vehicles are electric,
     rechargeable and could cost as low as $1 to $3 to run per day.

   Apple is also developing its own self-driving car.

     The project is code-named Titan and the vehicle design resembles a
     minivan, the Wall Street Journal reported… Apple already has
     technology that may lend itself to an electric car and expertise
     managing a vast supply chain. The company has long researched
     battery technology for use in its iPhones, iPads and Macs. The
     mapping system it debuted in 2012 can be used for navigation…

   And Uber is developing its own self-driving car.

     Uber said it will develop “key long-term technologies that advance
     Uber’s mission of bringing safe, reliable transportation to
     everyone, everywhere,” including driverless cars, vehicle safety and
     mapping services.

   It’s this last one that fully intends to transform the transportation
   landscape. Uber is going all-in on self-driving vehicles to the point
   it wants to entirely eliminate car ownership as a 20th century relic.

     Travis Kalanick, the CEO and founder of Uber, said at a conference
     last year that he’d replace human Uber drivers with a fleet of
     self-driving cars in a second. “You’re not just paying for the car —
     you’re paying for the other dude in the car,” he said. “When there’s
     no other dude in the car, the cost of taking an Uber anywhere
     becomes cheaper than owning a vehicle.” That, he said, will “bring
     the cost below the cost of ownership for everybody, and then car
     ownership goes away.”

   That’s the potential of self-driving cars — the outright extinction of
   car ownership. And with that, the elimination of entire industries
   built up around the existence of car ownership like: mechanics, car
   washes, parking, valets, body shops, rental companies, car insurance,
   car loans, and on and on. Even hugely expensive and capital intensive
   mass-transit infrastructure projects like streetcars and light rail can
   be dropped in favor of vastly cheaper on demand robotic “transportation
   clouds”, and all those construction and maintenance jobs right along
   with it.

   Big players are already in the game. There are huge savings to be
   found, huge profits to be created. Higher quality and safety is
   assured. Driverless vehicles are coming, and they are coming fast.

   But again, what about trucks specifically?

   Any realistic time horizon for self-driving trucks needs to look at
   horizons for cars and shift those even further towards the present.
   Trucks only need to be self-driven on highways. They do not need
   warehouse-to-store autonomy to be disruptive. City-to-city is
   sufficient. At the same time, trucks are almost entirely corporate
   driven. There are market forces above and beyond private cars operating
   for trucks. If there are savings to be found in eliminating truckers
   from drivers seats, which there are, these savings will be sought. It’s
   actually really easy to find these savings right now.

   Wirelessly linked truck platoons are as simple as having a human driver
   drive a truck, with multiple trucks without drivers following closely
   behind. This not only saves on gas money (7% for only two trucks
   together), but can immediately eliminate half of all truckers if for
   example 2-truck convoys became the norm. There’s no real technical
   obstacles to this option. It’s a very simple use of present technology.

   Basically, the only real barrier to the immediate adoption of
   self-driven trucks is purely legal in nature, not technical or
   economic. With self-driving vehicles currently only road legal in a few
   states, many more states need to follow suit unless autonomous vehicles
   are made legal at the national level. And Sergey Brin of Google has
   estimated this could happen as soon as 2017. Therefore…

   The answer to the big question of “When?” for self-driving trucks is
   that they can essentially hit our economy at any time.

The Eve of Massive Social and Economic Disruption

   Main Street USA has already taken a big hit, and increasingly so, over
   the past few decades. Manufacturing has been shipped overseas to areas
   where labor is far cheaper because costs of living are far cheaper.
   Companies like Walmart have spread everywhere, concentrating a reduced
   labor force into one-stop shopping facilities requiring fewer total
   workers than what was needed with smaller, more numerous, and more
   widely spread Mom & Pop type stores. Companies like Amazon have even
   further concentrated this even further reduced labor force into
   automated warehouse centers capable of obviating stores entirely and
   shipping directly to consumers.

   All of the above means fewer ways of securing employment in fewer
   places, while commerce has become more geographically concentrated and
   access to money has become increasingly shifted away from the bottom
   and middle of the income spectrum towards the top.
   Source: Mother Jones

   This is what happens when good-paying jobs are eliminated, and that
   money not spent on wages and salaries instead stays in the hands of
   owners of capital, or is given in smaller amounts to lower-paid
   employees in lower-wage jobs. Inequality grows more and more extreme
   and our land of opportunity vanishes. Economic growth slows to a crawl.

   This is where we’re at and this is what we face as we look towards a
   quickly approaching horizon of over 3 million unemployed truckers and
   millions more unemployed service industry workers in small towns all
   over the country dependent on truckers as consumers of their services.
   Glenrio, TX — Source: Reader’s Digest

   The removal of truckers from freeways will have an effect on today’s
   towns similar to the effects the freeways themselves had on towns
   decades ago that had sprung up around bypassed stretches of early
   highways. When the construction of the interstate highway system
   replaced Route 66, things changed as drivers drove right on past these
   once thriving towns. The result was ghost towns like Glenrio, Texas.

     With the patience that carved the Grand Canyon over eons, nature
     reclaims Glenrio, where the clock stopped with the bypass of Route
     66. The replacement of Route 66 with a four-lane superhighway that
     allowed motorists to zip past rather than wander through ultimately
     allowed Glenrio to decline.

   With self-driving cars and trucks, here again we face the prospect of
   town after town being zipped past by people (if even present) choosing
   to instead just sleep in their computer-driven vehicles. Except this
   time, there is no new highway being made for businesses to relocate
   closer to and new towns to emerge along. This time, as is true of the
   effect of technology on jobs, it’s different. This time, there’s no
   need for entire towns to even exist at all.

The Road Left to Take

   As close as 2025 — that is in a mere 10 years — our advancing state of
   technology will begin disrupting our economy in ways we can’t even yet
   imagine. Human labor is increasingly unnecessary and even economically
   unviable compared to machine labor. And yet we still insist on money to
   pay for what our machines are making for us. As long as this remains
   true, we must begin providing ourselves the money required to purchase
   what the machines are producing.

   Without a technological dividend, the engine that is our economy will
   seize, or we will fight against technological progress itself in the
   same way some once destroyed their machine replacements. Without
   non-work income, we will actually fight to keep from being replaced by
   the technology we built to replace us.

   Just as our roads a decade from now will be full of machine drivers
   instead of human drivers, a 21st century economy shall be driven by
   human consumers, not human workers, and these consumers must be freely
   given their purchasing power. If we refuse, if we don’t provide
   ourselves a universal and unconditional basic income soon, the future
   is going to hit us like a truck — a truck driven solely by ourselves.

   To allow this to happen would be truly foolish, for what is the entire
   purpose of technology but to free us to pursue all we wish to pursue?
   Fearing the loss of jobs shouldn’t be a fear at all. It should be
   welcomed. It should be freeing.

   No one should be asking what we’re going to do if computers take our
   jobs.

   We should all be asking what we get to do once freed from them.
     __________________________________________________________________

   Scott Santens writes about basic income on his blog. You can also
   follow him here on Medium, on Twitter, on Facebook, or on Reddit where
   he is a moderator for the /r/BasicIncome community of over 26,000
   subscribers.

   This article was written on a crowdfunded monthly basic income. If you
   found value in this article, you can support it along with all my
   advocacy for basic income with a monthly patron pledge of $1+.

   Are you a creative? Become a creator on Patreon. Join me in taking the
   BIG Patreon Creator Pledge for basic income.
     __________________________________________________________________

   Special thanks to Arjun Banker, Topher Hunt, Keith Davis, Albert
   Wenger, Larry Cohen, Danielle Texeira, Paul Wicks, Liane Gale, Jan
   Smole, Joe Esposito, Robert F. Greene, Martin Jordo, Victor Lau, Shane
   Gordon, Paolo Narciso, Johan Grahn, Tony DeStefano, Andrew Henderson,
   Erhan Altay, Bryan Herdliska, all my other funders for their support,
   and my amazing partner, Katie Smith.

   Would you like to see your name here too?
     __________________________________________________________________

   If you feel others need to read this article, please click “Recommend”

Basic Income

Articles about Universal Basic Income (UBI)

   (BUTTON) Follow

(BUTTON) 2.9K

   Some rights reserved

     * Basic Income
     * Self Driving Cars
     * Tech

(BUTTON) 2.9K claps

   (BUTTON)
   (BUTTON)
   Scott Santens

   Written by

Scott Santens

   (BUTTON) Follow

I no longer write on Medium. If you want to follow my writing, please
subscribe to scottsantens.news.

   (BUTTON) Follow
   Basic Income

Basic Income

   (BUTTON) Follow

Articles about Universal Basic Income (UBI)

   (BUTTON) Follow
   See responses (172)

Discover Medium

   Welcome to a place where words matter. On Medium, smart voices and
   original ideas take center stage - with no ads in sight. Watch

Make Medium yours

   Follow all the topics you care about, and we’ll deliver the best
   stories for you to your homepage and inbox. Explore

Become a member

   Get unlimited access to the best stories on Medium — and support
   writers while you’re at it. Just $5/month. Upgrade
   AboutHelpLegal
